% The document class supplies options to control rendering of some standard
% features in the result.  The goal is for uniform style, so some attention 
% to detail is *vital* with all fields.  Each field (i.e., text inside the
% curly braces below, so the MEng text inside {MEng} for instance) should 
% take into account the following:
%
% - author name       should be formatted as "FirstName LastName"
%   (not "Initial LastName" for example),
% - supervisor name   should be formatted as "Title FirstName LastName"
%   (where Title is "Dr." or "Prof." for example),
% - degree programme  should be "BSc", "MEng", "MSci", "MSc" or "PhD",
% - dissertation title should be correctly capitalised (plus you can have
%   an optional sub-title if appropriate, or leave this field blank),
% - dissertation type should be formatted as one of the following:
%   * for the MEng degree programme either "enterprise" or "research" to
%     reflect the stream,
%   * for the MSc  degree programme "$X/Y/Z$" for a project deemed to be
%     X%, Y% and Z% of type I, II and III.
% - year              should be formatted as a 4-digit year of submission
%   (so 2014 rather than the academic year, say 2013/14 say).

\documentclass[oneside,% the name of the author
                    author={Malak Hajji},
                % the degree programme: BSc, MEng, MSci or MSc.
                    degree={BSc},
                % the dissertation    title (which cannot be blank)
                    title={Designing An Accessible Computational Toolkit For Students},
                % the dissertation subtitle (which can    be blank)
                  subtitle={With Mixed Visual Abilities}]{dissertation}

\begin{document}

% =============================================================================

% This section simply introduces the structural guidelines.  It can clearly
% be deleted (or commented out) if you use the file as a template for your
% own dissertation: everything following it is in the correct order to use 
% as is.

\section*{Prelude}
\thispagestyle{empty}

A typical dissertation will be structured according to (somewhat) standard 
sections, described in what follows.  However, it is hard and perhaps even 
counter-productive to generalise: the goal is {\em not} to be prescriptive, 
but simply to act as a guideline.  In particular, each page count given is
important but {\em not} absolute: their aim is simply to highlight that a 
clear, concise description is better than a rambling alternative that makes
it hard to separate important content and facts from trivia.

You can use this document as a \LaTeX-based~\cite{latexbook1,latexbook2} 
template for your own dissertation by simply deleting extraneous sections
and content; keep in mind that the associated {\tt Makefile} could be of
use. %, in particular because it automatically executes \mbox{\BibTeX} to 
deal with the associated bibliography. 
Alternatively, upload this template, dissertation.bib, dissertation.cls, 
dtklogos.sty and the "logo" folder to Overleaf (an online \LaTeX editor and compiler) and work on your thesis there.

\textbf{Do not include this section in your final dissertation --- just delete it from the source.}

% =============================================================================

% This macro creates the standard UoB title page by using information drawn
% from the document class (meaning it is vital you select the correct degree 
% title and so on).

\maketitle

% After the title page (which is a special case in that it is not numbered)
% comes the front matter or preliminaries; this macro signals the start of
% such content, meaning the pages are numbered with Roman numerals.

\frontmatter


%\lstlistoflistings

% The following sections are part of the front matter, but are not generated
% automatically by LaTeX; the use of \chapter* means they are not numbered.

% -----------------------------------------------------------------------------

\chapter*{Abstract}

{\bf A compulsory section, of at most 300 words} 
\vspace{1cm} 

\noindent
This section should pr\'{e}cis the project context, aims and objectives,
and main contributions (e.g., deliverables) and achievements; the same 
section may be called an abstract elsewhere.  The goal is to ensure the 
reader is clear about what the topic is, what you have done within this 
topic, {\em and} what your view of the outcome is.

The former aspects should be guided by your specification: essentially 
this section is a (very) short version of what is typically the first 
chapter. If your project is experimental in nature, this should include 
a clear research hypothesis.  This will obviously differ significantly
for each project, but an example might be as follows:

\begin{quote}
My research hypothesis is that a suitable genetic algorithm will yield
more accurate results (when applied to the standard ACME data set) than 
the algorithm proposed by Jones and Smith, while also executing in less
time.
\end{quote}

\noindent
The latter aspects should (ideally) be presented as a concise, factual 
bullet point list.  Again the points will differ for each project, but 
an might be as follows:

\begin{quote}
\noindent
\begin{itemize}
\item I spent $120$ hours collecting material on and learning about the 
      Java garbage-collection sub-system. 
\item I wrote a total of $5000$ lines of source code, comprising a Linux 
      device driver for a robot (in C) and a GUI (in Java) that is 
      used to control it.
\item I designed a new algorithm for computing the non-linear mapping 
      from A-space to B-space using a genetic algorithm, see page $17$.
\item I implemented a version of the algorithm proposed by Jones and 
      Smith in [6], see page $12$, corrected a mistake in it, and 
      compared the results with several alternatives.
\end{itemize}
\end{quote}

% -----------------------------------------------------------------------------


\chapter*{Dedication and Acknowledgements}

{\bf A compulsory section}
\vspace{1cm} 

\noindent
It is common practice (although totally optional) to acknowledge any
third-party advice, contribution or influence you have found useful
during your work.  Examples include support from friends or family, 
the input of your Supervisor and/or Advisor, external organisations 
or persons who  have supplied resources of some kind (e.g., funding, 
advice or time), and so on.

% =============================================================================

% After the front matter comes a number of chapters; under each chapter,
% sections, subsections and even subsubsections are permissible.  The
% pages in this part are numbered with Arabic numerals.  Note that:
%
% - A reference point can be marked using \label{XXX}, and then later
%   referred to via \ref{XXX}; for example Chapter\ref{chap:context}.
% - The chapters are presented here in one file; this can become hard
%   to manage.  An alternative is to save the content in seprate files
%   the use \input{XXX} to import it, which acts like the #include
%   directive in C.


% -----------------------------------------------------------------------------


\chapter*{COVID-19 Statement}

{\bf An optional section, of at most 800 words} 
\vspace{1cm} 

\noindent
A summary of any planned research activities disrupted by Covid-19 restrictions and the extent to which it was possible to adapt the work in those changed circumstances. If the project was able to go forward as planned, you can safely remove this section without losing any marks. The following may be included:

\begin{itemize}
\item Details of any planned research activities curtailed by the pandemic because of, for example, lack of access to facilities, libraries, archives, research participants, fieldwork, etc. Information on any curtailed training should be included only insofar as it relates to the impact on research activities and on the dissertation.

\item An acknowledgement of the anticipated contribution and value to the dissertation if those research activities had not been curtailed and what was possible to include in the dissertation in the circumstances, including where alternative choices were made to adapt the work and whether there are any weaknesses that could not be overcome.

\item Any other relevant factors on the impact of Covid-19 on research activities and on the contents of the dissertation.

\item Details of any research activities required by the examiners as part of a resubmission that were curtailed by the pandemic may be included in a new or revised Covid-19 statement in the resubmitted dissertation.
\end{itemize}

% -----------------------------------------------------------------------------

% This macro creates the standard UoB declaration; on the printed hard-copy,
% this must be physically signed by the author in the space indicated.

\makedecl



% -----------------------------------------------------------------------------

% LaTeX automatically generates a table of contents, plus associated lists 
% of figures and tables.  These are all compulsory parts of the dissertation.

\tableofcontents
\listoffigures
\listoftables

% -----------------------------------------------------------------------------



\chapter*{Ethics Statement}

{\bf A compulsory section} 
\vspace{1cm} 

In almost every project, this will be one of the following statements:
    \begin{itemize}
        \item ``This project did not require ethical review, as determined by my supervisor, [fill in name]''; or
        \item ``This project fits within the scope of ethics application 0026, as reviewed by my supervisor, [fill in name]''; or
        \item ``An ethics application for this project was reviewed and approved by the faculty research ethics committee as application [fill in number]''.
    \end{itemize}
    
See Section 3.2 of the unit Handbook for more information. If something went wrong and none of those three statements apply, then you should instead explain what happened.


% -----------------------------------------------------------------------------

\chapter*{Summary of Changes}

{\bf A conditional section} 
\vspace{1cm} 

If and only if the dissertation represents a resubmission (e.g., as the result of
a resit), this section is compulsory: the content should summarise all
non-trivial changes made to the initial submission.  Otherwise you can
omit it, since a summary of this type is clearly nonsensical.

When included, the section will ideally be used to highlight additional
work completed, and address criticism raised in any associated feedback.
Clearly it is difficult to give generic advice about how to do so, but
an example might be as follows:

\begin{quote}
\noindent
\begin{itemize}
\item Feedback from the initial submission criticised the design and 
      implementation of my genetic algorithm, stating ``there seems 
      to have been no attention to computational complexity during the
      design, and obvious methods of optimisation are missing within
      the resulting implementation''.  Chapter $3$ now includes a
      comprehensive analysis of the algorithm, in terms of both time
      and space.  While I have not altered the algorithm itself, I
      have included a cache mechanism (also detailed in Chapter $3$)
      that provides a significant improvement in average run-time.
\item I added a feature in my implementation to allow automatic rather
      than manual selection of various parameters; the experimental
      results in Chapter $4$ have been updated to reflect this.
\item Questions after the presentation highlighted a range of related
      work that I had not considered: I have make a number of updates 
      to Chapter $2$, resolving this issue.
\end{itemize}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Supporting Technologies}

{\bf An optional section}
\vspace{1cm} 

\noindent
This section should present a detailed summary, in bullet point form, 
of any third-party resources (e.g., hardware and software components) 
used during the project.  Use of such resources is always perfectly 
acceptable: the goal of this section is simply to be clear about how
and where they are used, so that a clear assessment of your work can
result.  The content can focus on the project topic itself (rather,
for example, than including ``I used \mbox{\LaTeX} to prepare my 
dissertation''); an example is as follows:

\begin{quote}
\noindent
\begin{itemize}
\item I used the Java {\tt BigInteger} class to support my implementation 
      of RSA.
\item I used a parts of the OpenCV computer vision library to capture 
      images from a camera, and for various standard operations (e.g., 
      threshold, edge detection).
\item I used an FPGA device supplied by the Department, and altered it 
      to support an open-source UART core obtained from 
      \url{http://opencores.org/}.
\item The web-interface component of my system was implemented by 
      extending the open-source WordPress software available from
      \url{http://wordpress.org/}.
\end{itemize}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Notation and Acronyms}

{\bf An optional section}
\vspace{1cm} 

\noindent
Any well written document will introduce notation and acronyms before
their use, {\em even if} they are standard in some way: this ensures 
any reader can understand the resulting self-contained content.  

Said introduction can exist within the dissertation itself, wherever 
that is appropriate.  For an acronym, this is typically achieved at 
the first point of use via ``Advanced Encryption Standard (AES)'' or 
similar, noting the capitalisation of relevant letters.  However, it 
can be useful to include an additional, dedicated list at the start 
of the dissertation; the advantage of doing so is that you cannot 
mistakenly use an acronym before defining it.  A limited example is 
as follows:

\begin{quote}
\noindent
\begin{tabular}{lcl}
AES                 &:     & Advanced Encryption Standard                                         \\
DES                 &:     & Data Encryption Standard                                             \\
                    &\vdots&                                                                      \\
${\mathcal H}( x )$ &:     & the Hamming weight of $x$                                            \\
${\mathbb  F}_q$    &:     & a finite field with $q$ elements                                     \\
$x_i$               &:     & the $i$-th bit of some binary sequence $x$, st. $x_i \in \{ 0, 1 \}$ \\
\end{tabular}
\end{quote}

% -----------------------------------------------------------------------------

\mainmatter
\chapter{Introduction}
\label{chap:context}

{\bf Unlike the frontmatter up to and including the Summary of Changes, which you should not deviate from, Chapters 1--5 represent a suggested outline only. This outline will only be appropriate for a specific type of project. You should talk with your supervisor about the best way to structure your own dissertation, but ultimately the choice is yours. However, almost every project will want to include the content discussed in these chapters in some way. For more advice on structuring your dissertation, see the unit handbook.}
\vspace{1cm} 
\section{Problem}
\section{Approach}
\section{Aims}


\noindent
This chapter should introduce the project context and motivate each of the proposed aims and objectives.  Ideally, it is written at a fairly high-level, and easily understood by a reader who is technically competent but not an expert in the topic itself.

In short, the goal is to answer three questions for the reader.  First, what is the project topic, or problem being investigated?  Second, why is the topic important, or rather why should the reader care about it?  For example, why there is a need for this project (e.g., lack of similar software or deficiency in existing software), who will benefit from the project and in what way (e.g., end-users, or software developers) what work does the project build on and why is the selected approach either important and/or interesting (e.g., fills a gap in literature, applies results from another field to a new problem).  Finally, what are the central challenges involved and why are they significant? 
 
The chapter should conclude with a concise bullet point list that summarises the aims and objectives.  For example:

\begin{quote}
\noindent
The high-level objective of this project is to reduce the performance 
gap between hardware and software implementations of modular arithmetic.  
More specifically, the concrete aims are:

\begin{enumerate}
\item Research and survey literature on public-key cryptography and
      identify the state of the art in exponentiation algorithms.
\item Improve the state of the art algorithm so that it can be used
      in an effective and flexible way on constrained devices.
\item Implement a framework for describing exponentiation algorithms
      and populate it with suitable examples from the literature on 
      an ARM7 platform.
\item Use the framework to perform a study of algorithm performance
      in terms of time and space, and show the proposed improvements
      are worthwhile.
\end{enumerate}
\end{quote}

% -----------------------------------------------------------------------------

\chapter{Contextual Background}
\label{chap:technical}

\section{Introduction}

Computer science and programming has become greatly beneficial in everyday life and introducing computing in education for all young children has now become essential for several reasons. Learning how to program enables young students to actively engage with computers and also teaches them strategies to break down, analyse and solve problems by being logical and creative [26]. In fact, previous studies show that it is crucial for children to engage in computing education as early as possible, since stereotypes about who can or cannot code tend to form at a young age [19, 21]. 
Computing concepts are first introduced as computational thinking skills in primary school. By learning computational thinking at a young age, children are better prepared to assimilate programming languages later on in their education. 
In this section, I will discuss the motivations behind computational thinking learning and give an overview of current computational tools used to teach CT in UK schools. I will then explore how computing is taught to learners with mixed visual abilities, to determine the challenges they are faced with and analyse the different inclusive computational learning tools and their limitations. By combining the background on computational thinking learning and current limitations with the inclusive environments, I aim to design a fully accessible block-based and robotic coding environment for mixed visual abilities.

\section{Learning Computational Thinking}  

Computational thinking is becoming a fundamental skill for everyone, not just computer scientists, and just like reading, writing and arithmetic, computational thinking should be a part of every child’s analytical ability. It is defined as “the thought process involved in formulating a problem and expressing it in a way that a computer –human or machine- can effectively carry out.” [wing]. CT draws on the concepts fundamental to computer science, such as sequences and operators as well as practices like logical reasoning, testing, debugging and abstraction. The value of CT does not just lie in computing, it can be used to support problem solving across all disciplines, including the humanities, math and science [google for education, ct course]. Therefore, acquiring CT as a skill impacts the social, emotional and cognitive development of children, while encouraging personal and career growth.


\subsubsection{Low floor, high ceiling and wide walls}

Since CT learning has become widely spread, it prompted researchers and designers to develop an abundance of new toolkits and computing environments that are enjoyable and accessible to increasingly younger learners. The idea of “low floor, high ceiling and wide walls” is one of the guiding principles for the creation of these environments for children. It essentially means that it should be easy for a beginner to cross the threshold to create working programs (low floor) and the tool should be extensive enough to satisfy the needs of advanced programmers (high ceiling).  Wide walls have been described as one of the most crucial features for computational thinking tools as they enable diverse exploration and encourage creativity [56]. Computational toolkits have been widening the walls by increasing the number and type of outputs supported and by enabling children to move beyond a specific set of “recommended activities”. 

Several programming tools used to teach computational thinking fit these criteria to varying degrees. Some of the most widely used graphical programming environments in education are Scratch, Blockly; and robotics kits and tangible media such as Arduino and BeeBot. These tools are very efficient in learning computational thinking because they are relatively easy to use and allow young students to focus on designing and creating, while avoiding issues of programming syntax. Several of these introductory computational tools use the three-stage “use–modify–create” progression to help the learner go from user to modifier to creator of computational artifacts (Lee et al., 2011), which supports the criteria of 'Low floor, high ceiling and wide walls.' 

Other activities such as game design and robotics have also been utilised as a means for the iterative exploration of CT. They are not only motivating and engaging school children but also introducing them to computer thinking. 

To summarise, the aforementioned visual and tangible programming experiences are very useful in introducing computational thinking skills at a young age. These experiences cater for all programming levels and encourage creativity since several of the utilised tools are guided by the principle of 'low floor, high ceiling and wide walls'. The introduction to computational thinking is often followed by exposure to high-level programming languages such as Python and Java.


\subsubsection{Computational Thinking in UK Schools}

The UK curriculum provides a concrete example of the adaptation of computational thinking learning in early childhood. The introduction of computing into the national curriculum for England in 2014, brought with it the requirement for primary school children to be taught computational thinking skills from the age of five (Department for Education, 2014), 
The introduction of computational thinking at a younger age has been very beneficial and it has proven to establish confidence in technology early on, which can assist in encouraging children to pursue Computer Science in higher education.

In Key Stage 1, computational thinking skills are developed through the introduction of floor robots such as the BeeBot and the LOGO turtle. It is expected that learners will identify what each floor robot command does and use that knowledge to design an algorithm to move their robot around the mat. [https://teachcomputing.org/curriculum/key-stage-1/programming-a-moving-a-robot]. Students are thus introduced to the early stages of program design using algorithms which is considered a computational thinking concept. This is achieved by learners outlining what their task is by identifying the starting and finishing points of the robot's route. This outlining will ensure that learners clearly understand what they want their program to achieve. [ozobot 7].

In Key Stage 2, students are introduced to visual languages such as Scratch to allow them to experiment with more complex programming concepts such as loops and conditional statements. (// rewrite Visual languages are primarily utilised at this age, as they allow students to code without requiring complex syntax or types to be learnt[7]. In addition to this, visual languages are best suited to their intellectual and cognitive development at this age[32][56]. The concepts taught through visual languages directly translate to text- based languages and provide a base set of skills that can be developed further.//

\section{Visual Programming Languages}

In this section, I will discuss the motivations behind the learning of computational thinking through the use of visual programming environments and languages in primary schools.
I will then discuss the limitations in these environments, particularly for students with visual impairments, to design a suitable alternative.

\subsection{Types of Languages and Benefits}
\subsubsection{Block-based languages}

Computer programming can be a way of teaching computational thinking and
most research scenarios consider block-based programming languages with graphical outputs, such as Scratch, a direct and defined strategy to teach young children computational concepts[8 filipa]. Block-based languages tend to lead to quicker engagement and early ‘quick steps forward’ These platforms often introduce many programming activities that can be personalized and modified to the user's interests, which can keep children engaged and motivated to build their own projects. Children can also tinker with the different digital blocks and put together sequences of instructions, as opposed of having to learn how to read the syntax from traditional visual programming languages. This allows young students to build their own ideas and gain confidence in their novice programming skills while growing an interest in learning new more complex computational concepts. [ in word document]


[// to add if needed // Blocks programming languages [21] can allow beginners to construct programs without struggling with the syntax, making text-based programming more comfortable to learn in the future. These graphical programming languages have proven to be more engaging and understandable for new learners. There are learning barriers that block programming languages that attempt to minimize: selection, use, and coordination.
When constructing a program with a blocks programming language, users can rely on recognition of the categories and shape of the block instead of recalling vocabulary. Block-based code reduces the cognitive load a new programmer has to learn by simplifying the syntax grouping computational patterns into blocks. Since blocks can only be assembled in a certain way or with other specific types of blocks, the code is less likely to be syntactically incorrect. These characteristics and designs simplify the discovery and exploration of building programs for new learners that can concentrate on the meaning of the code instead of the writing notation.]

\subsubsection{Block and Icon based languages}
Previous research has shown that children as young as four years old can understand basic computer programming concepts and build and program robotics projects, however, it is hard for them to understand blocks with written statements such as the blocks in Scratch. Thus, icons in block programming have been widely used to introduce computing concepts to younger demographics. Icons are easily recognised by children, as they relate to everyday activities and make the interface easier to understand.
An example of such a platform is ScratchJr [26 filipa], the redesign of the Scratch platform for the developmental and learning needs of children from kindergarten to second grade. Children can put together blocks identified with icons to form instructions and have a graphical output, thus eliminating the need to read blocks while maintaining the user engaged.

\subsubsection{Text-based languages}

Text-based languages are less utilised to teach younger children to teach computational thinking as it requires the children to be familiar with the syntax. It is important to note that the UK primary computing curriculum is not about teaching children a specific programming language, rather than understanding the principals of programming and applying these to solve specific problems. Thus, unlike KS3, where the national curriculum states that students should use a text based programming language, this is not a requirement in primary schools, and generally block-based programming environments are used.

\subsubsection{Impact on Future Education }

A few studies have been conducted to look at the transition from block-based and graphical languages used in primary schools to text-based languages later on in education. One study looked at students who had taken Scratch programming classes in middle school and examined if they performed better in a high school, text-based programming course. The researchers were able to find some areas where the Scratch students outperformed their peers. The Scratch students specifically had a better grasp and understanding of computational concepts such as the concept of looping. Additionally, the authors found qualitative differences between the two groups, with students who had prior Scratch experience reporting high levels of self efficacy and motivation to learn how to code. [[https://www.proquest.com/docview/1826352865/fulltextPDF/865C532C9AE24508PQ/1?accountid=9730]]

Another study demonstrated that a blended approach between block-based and text-based languages enables students to better understand the high-level programming concepts rather than focus on the low-level compilation errors. Students were more confident using the text-based environment after gaining familiarity with the block-based language. [ https://hal.inria.fr/hal-01625380/document]
 
\subsection{Limitations}

Visual Programming Languages are inaccessible to and visually impaired and motorically challenged individuals [ https://dl.acm.org/doi/pdf/10.1145/2212776.2223757]]. Although they allow users to create programs by arranging graphical blocks logically, such visual languages are completely dependent on the use of a mouse and keyboard, which makes it impossible for students with a mobility impairment to use them independently. Visually impaired students may also find it hard to assemble programs due to the drag and drop nature of assembling code in most systems, specifically if they are unable to track the mouse on the screen.  Additionally, the majority of VPLs are incompatible with screen readers. Thus, visually impaired pupils have to rely on a sighted peer or teaching assistant to describe the programming blocks[46 ozobot]. This takes away a student’s independence when coding and isolates the student from their sighted peers in the case of mainstream schools.


\section{Computing in Special Education}
In this section, I define the scope of the project by describing how children with mixed visual abilities are taught in the UK.  I then discuss existing tools for computational learning and physical computing in special educational settings. 

\subsection{Teaching Learners with Mixed Visual Abilities}

In the UK, more than 2 million people are living with sight loss and approximately 25,660 of children up to the age of 16 have a vision impairement. The majority of blind and low vision children are taught in mainstream schools (RNIB, 2016). Learners are supported by qualified teachers of the visually impaired (QTVI) who help children adapt to the curriculum. There are also a very few specialist schools that work with blind and low vision students who have an additional or profound disability. This variety suggests that the mainstream national curriculum needs to work across a wide range of children abilities. 
The mainstreaming of learners with disabilities was driven by a vision of social inclusion supported by inclusive teaching practices (UK Parliment, 1981). 
In practice, adopting an inclusive approach in education requires the use of multisensory teaching technique, to ensure that all pupils can participate regardless of their abilities. However, the lack of resources and time on the part of teachers, paired with new non-accessible and non-inclusive technologies in schools, have made the implementation of mainstream education for disabled children full of challenges, impacting their academic and social participation (Gray, 2009). 

As a result, the focus has shifted to addressing the more urgent need of making in-class materials accessible and inclusive. This has been particularly challenging for STEM (Science, Technology, Engineering and Math) subjects (Moon, Todd, Morton, & Ivey, 2012). There have been several tried approaches to tackle this issue in STEM, for example, one suggestion has been to pair disabled and non-disabled students and divide the work by abilities. The sighted learner manipulates the programming environment such as Scratch, while a visually impaired child suggests ideas for an animation that he cannot experience. This method however leads to partial or even non- participation of the visually impaired pupils. Alternatively, teaching assistants (TA) are often asked to bridge the gap and make their own adaptations to create accessible computational tools. This approach often leads to focused interactions between TA and student and creates an assistance bubble that isolates disabled children from the rest of the class (Metatla & Cullen, 2018). Thus, there is a great disparity between the envisioned social inclusion in education and the reality of teaching computing. 

\subsection{Inclusive Computational Learning Tools}

A range of tools have been developed to support computational learning by learners with mixed visual abilities and to bridge the gap in computing in special education. Most of these tools aim to make code structures apparent in order to make existing programming languages more accessible, e.g. (Baker, Milne, & Ladner, 2015), or interoperable with assistive technology such as screen readers or magnification software [ref], e.g. (Ludi, Ellis, & Jordan, 2014). In this section, I will consider both advantages and limitations of some accessible computational tools to influence my system’s design decisions. 


\subsubsection{Screen Readers}
A screen reader is a software application that allows a visually impaired user to read and navigate a computer screen. It can be used on both computers and touchscreen-based smartphones or tablets. Several devices possess built-on screen readers, for example VoiceOver for macOS and iOS, TalkBack for Android. Screen readers are also available through third-party developers such as JAWS and NVDA for Windows PCs. 
Users can utilise the keyboard or touch to change the focus of a screen reader to different elements on the screen, and then the focused on element is either read aloud or read on a refreshable Braille display. If the item is interactive, (e.g. a button or link), the user can interact with it via keyboard commands or touch gestures.

\subsubsection{Quorum}
Quorum is an accessible evidence-based programming language. The main aim of Quorum is to simplify syntax to provide accessibility for visually impaired students. The simplified syntax helps students better understand code as it is read aloud using screen readers or displayed on a braille device. Quorum can be an effective tool for those who already know how to code, but is less suitable for novices. Like many other text-based languages, it can be challenging to teach to younger students to use Quorum because they have to be familiar with the language syntax and keywords.

\subsubsection{JBrick}
Ludi et al. [41] presented JBrick, a technology that simplified the browsing and entry of text commands and code compilation through visual and audio-feedback, which helps visually impaired teenagers program Lego robots. The system works with JAWS, the aforementioned screen reader for Windows, and ZoomText, a software for screen magnification. Thus, the JBrick environment requires students to be proficient with screen readers and is more suitable for older visually impaired students.

\subsubsection{Javaspeak}
Smith et al. [61] introduced JavaSpeak, an editor providing additional information about the structure and semantics of written Java code, to assist computer science majors with VI to learn how to program in Java. Similarly to JBrick, Javaspeak  relies on screen readers to read the code structure. This system is designed for university students and is not suitable for younger children who have limited knowledge with semantics and are not fully proficient in using a screen-reader and keyboard.

\subsubsection{Blocks4All}

Blocks4All is an accessible block-based programming environment[45][44].The application was developed for a touchscreen Apple iPad due to the built-in screen reader VoiceOver and zoom capabilities. It features puzzle shaped blocks that fit together by the shape of the design and by the audio feedback from the app. The system is an audio-based drag and drop system, so the user receives audio feedback describing the blocks locations to help move them. The Blocks4All environment is used to control a Dash robot as an tangible output for visually impaired children. This allows students to feel and hear the output of thee programmed commands through the movement or sounds made by the Dash robot. As Blocks4All heavily relies on audio representation using VoiceOver, the system may not be suitable for children who are deaf. In addition to this, most visually impaired students prefer spatial representation to audio representation since they use a spatial mental model to remember information. [ref https://dl.acm.org/doi/pdf/10.1145/3173574.3173643]

\subsubsection{StoryBlocks}
StoryBlocks [13,15] is a tangible programming toolkit that was designed for VI users to create audio stories. The system is composed of blocks that represent story components such as characters and actions. The system captures the blocks with a camera when they are put together on the workspace and outputs a translated audio story. The blocks are all identifiable by icons with distinct shapes and colors. The shape of each block indicates how it can be connected to other blocks and is marked with a distinctive dot pattern to be recognized by the program running on the mobile device. StoryBlocks was designed as a combination of tangible programming blocks and audio output, which is a compelling way to teach basic computational concept, particularly for visually impaired individuals.

\subsubsection{Torino}
Torino [16] is a physical programming language to teach computational thinking to children between 7 and 11 years old with mixed visual abilities. It is one of the first systems made to teach basic programming to primary school students. The system is composed of plastic pods acting as instruction beads that can be physically connected and manipulated to generate sound as output. There are three types of beads: play, pause, and loop, and they each translate to a line of code in the program. Each bead further has buttons to control the repetition of the sound to be played, or to increase or decrease the length of pause and play executions, introducing the concept of variables. There is also a Code Jumper hub unit which allows the user to press play and pause on execution and read aloud the code. Students can connect pods to each other and to the hub by the use of jumper cords, creating programs with sound output. The Code Jumper app can connect to the hub unit to view the code visual format and it is also made accessible for screen readers and magnification software.  

The exploration of the beads is  critical for VI children to understand their functionalities. Thus, to facilitate their identification, each pod is distinct in size and shape, and is colored using high contrasting colors. In the end, the creators of Code Jumper were able to infer that touch, audio feedback, and visual representation are essential in the design of inclusive tools for VI children.
However, this system is unpractical for classroom use. It is composed of costly custom electronic components, and may not be within the budget of many schools. Broken equipment will also be hard and expensive to replace or repair. Since it relies on audio output, Code Jumper is also unsuitable for children who are deaf or hard of hearing as they may be unable to understand the output of the system. [ ref https://dl.acm.org/doi/pdf/10.1145/3064663.3064689]

\subsubsection{}


\section{Ozobot}

\noindent
This chapter is intended to describe the background on which execution of the project depends. This may be a technical or a contextual background, or both. The goal is to provide a detailed explanation of the specific problem at hand, and existing work that is relevant (e.g., an existing algorithm that you use, alternative solutions proposed, supporting technologies).  

Per the same advice in the handbook, note there is a subtly difference from this and a full-blown literature review (or survey).  The latter might try to capture and organise (e.g., categorise somehow) \emph{all} related work, potentially offering meta-analysis, whereas here the goal is simple to ensure the dissertation is self-contained.  Put another way, after reading this chapter a non-expert reader should have obtained enough background to understand what \emph{you} have done (by reading subsequent sections), then accurately assess your work against existing relevant related work.  You might view an additional goal as giving the reader confidence that you are able to absorb, understand and clearly communicate highly technical material and to situate your work within existing literature.


% -----------------------------------------------------------------------------

\chapter{Related Work}
\label{chap:execution}
In this chapter, I will explore tools with similar features to the system I am building, to determine their successes and limitations. The tools studied include examples of tangible programming tools and robots used to teach computing concepts to young students. I will also make a comparison of their features and other characteristics such as output types, price, and accessibility, to draw out the limitations they could pose for visually impaired students. From this discussion, I will then aim to develop an initial set of required features to design a system that is accessible to mixed visual abilities.

\section{Tangible Tools for Learning}

Tangible Tools or Tangible User Interfaces (TUI) are an alternative to visual programming for younger learners. They are composed of physical pieces, that are often puzzle-like pieces, that are put together to generate code. Tactile tools often use icons to convey the meaning of the physical piece. This allows the tool to be universally understood and suitable for use with younger children as previously described. Tactile learning methods have been praised for their ability to aid students in understanding concepts. As young children have a deep and implicit spatial knowledge, it is more intuitive for them to grasp computational concepts by moving physical pieces to manipulate another body i.e a robot. Thus, tangible tools such as LOGO and BeeBot are widely used in KS1 and KS2 to teach computational thinking. [https://www.nfer.ac.uk/publications/FUTL69/FUTL69.pdf]
In order to identify features that make tangible tools accessible, I have explored several commercially available tangible toolkits used to teach young children computational thinking through coding.

\subsection{Examples of TUI}
\subsubsection{Osmo}
Osmo is a mobile application that utilises a mirror attached to the Apple iPad to capture tangible blocks placed on the workspace in front of the iPad. These blocks are detected by the system and used to play different games and display visual effects on the screen. Children can assemble coding blocks on the work surface to control a character on the screen on the Osmo Coding Awbie game. The blocks instruct the robot like character to traverse an environment. The output and feedback from this system is purely visual and is not compatible with screen readers. Additionally, the blocks themselves only show visual information and do not have embossing for a tactile experience, making the system unsuitable for children with a visual impairment.  

\subsubsection{Cubetto}
Cubetto is a tangible programming language designed for students in preschool[11]. The Cubetto [31] tool consists of a robot, a programming tray with slots, and four categories of blocks: forward, right, left, and function. Children produce programs by inserting the action blocks in the programming tray. The resulting programs aim to get the Cubetto robot to traverse the map mat to the desired location. The whole system is only composed of physical parts, with no need for screens and it provides both tactile input and output, which would be suitable for students with visual impairment. However, the blocks are not non-visually distinguishable, and the map itself is purely visual. Without collaborating with a sighted assistant, the system is unsuitable for use with VI individuals. In addition to this, the Cubetto may be too expensive for a classroom budget to replace if components are broken.  

\subsubsection{KIBO}
KIBO [32 filipa] is a screen-free robotic kit that involves hardware parts to assemble and tangible blocks to program the robot. The tangibles are wooden blocks with drawings and text on top of them to identify their function. After constructing the code sequence, children can scan the barcode on the block with the robot. The robot then follows the instructions coded.  

\subsubsection{Algobrix}
Algobrix is a screen-free toolkit that is composed of a robot and tangible blocks. The blocks are distinct in shape and color and have embossing which makes them tactically recognisable for visually impaired students. It is used to teach different computational concepts such as loops, algorithmic thinking and multithreading. 

\subsubsection{Matatalab}
Matatalab Coding Set is a hands-on computational toolkit that includes a robot, tangible blocks, and a mat. The children program the robot by putting together a set of blocks on the workspace. The system then scans the blocks and sends the program using Bluetooth to the robot. Matatalab is used in some classrooms in other countries and it has proven to teach fundamental coding skills and develop necessary cognitive abilities through educational coding games from a young age without the need for screen and literacy. 

The following table shows a comparison of these different TUI and their features.


\subsection{Limitations}
When examining the examples of currently publicly available TUI on table (num), it can be concluded that most tactile tools have their limitations. None of the presented tools possess features that could make it an accessible, tangible programming tool for the visually impaired. This is partly due to the fact that most of the tools do not possess tactically distinguishable blocks. The blocks do not have embossing or characteristics that the children can recognise, except for Cubetto and Algobrix. 

Additionally, most of the tools do not teach all the necessary computational concepts, as they do not support more complex programming concepts. Thus, most of the TUIs are used for introductory programming and the complexity of programming ranges in the beginner stages, not allowing more experienced children to evolve and break that barrier, which does not support the 'low floor, high ceiling' concept. 

Finally, the explored tangible tools are all very expensive which potentially makes them out of budget for a lot of schools, it is therefore very unlikely for students to be able to experience coding using tangible interfaces such as the ones presented in table num.

\section{Robots for Learning}

After examining different TUIs, it can be concluded that the most popular output of the programming action is the behavior of a robot. In this section, I will explain the motivation behind using a robot in a TUI to teach computing and give context on the robot I will be using in my system, the Ozobot.

\subsection{Benefits}
- sense of control, spatial output, with feedback.

There are research scenarios exploring children robot interaction in different areas. For example, Papert, who is famous for the development of the Logo programming environment for children, observed that young children have a deep and implicit spatial knowledge based on their own personal sensorimotor experience of moving through a three- dimensional world. He argued that by allowing children to manipulate another body such as a robot, they might gradually develop increasingly more explicit representations of control structures for achieving effects such as creating graphical representations on a screen. [https://www.nfer.ac.uk/publications/FUTL69/FUTL69.pdf] This also applies to the interaction between visually impaired people and robots. 
// rewrite Researchers are exploring how blind people perceive robots and how these devices fit in their expectations and fears regarding the increase of dependency in them [23]. The researchers could infer from the answers that the participants preferred to feel in control. If they were to be in contact with a robot, they preferred that it responded to commands and questions. While interacting with a robot, it was helpful to the users that it made a sound, allowing them to identify what was its position and state. // 

\subsection{Ozobot}



\section{Discussion} 
have a list of requirements from everything discussed.
have a table of all discussed systems 




% -----------------------------------------------------------------------------

\chapter{Technical Background}
\label{chap:evaluation}



\noindent
In this chapter, I will be discussing the technologies I used while building my system. It involves a combination of:

\begin{enumerate}
\item Open-source fiducial marker generation and detection libraries.
\item OpenCV libraries.
\item Python GUI frameworks. 
\item Alternative Ozobot programming languages.
\end{enumerate}

\noindent
These elements all interact within the system to eventually program the Ozobot to complete a set of actions. This interaction occurs as follows.
 Firstly, the fiducial markers are placed on top of the physical programming blocks. It is important to note that each marker is unique to a block and the action depicted on that block. Secondly, the system uses OpenCV libraries to detect the markers on the block in real time video stream. The system has two different outputs depending on the action chosen. If the user chooses to translate the blocks in the image to action performed by the Ozobot, the detection algorithm returns a set of ID’s associated to each marker that my code translates to a list of actions. Finally, using an alternative Ozobot programming language called Ozopython, the system converts the list of actions to a sequence of flashing lights that the Ozobot can interpret and then perform the dictated actions. 
Alternatively, the user can choose the ‘Recognise Block’ action. This is a separate action that does not program the Ozobot, instead the system uses the OpenCV fiducial marker detection algorithm to detect the blocks and play an audio file describing the action that the block represents. 
Additionally, by using the Python GUI framework Tkinter, I am facilitating the user’s interaction with my system. All commands and information are accessible from the Tkinter window, and the buttons displayed on it. 
In this chapter, I aim to provide a general understanding of the open-source libraries and repositories I have used in my system and to justify the technical choices I have made, before providing an in-depth technical system design in chapter 5.

\section{Fiducial markers}
\noindent
The functioning of my system relies on the interpretation of the sequence of programming blocks. Hence, to ensure that the blocks are accurately detected, I have made use of fiducial markers. The camera is able to recognize the set of instructions through the AruCo marker encoding on each block. 
I have considered alternative approaches such as incorporating a barcode scanner in the block or using a shape detector, but all these approaches often provide inaccurate results. 
A fiducial marker system consists of a set of generated patterns that can be detected by a computer equipped with a camera and an appropriate marker detection algorithm.  Their design enables the markers to be physically small or be positioned at a large distance from the camera, while still being accurately detected in low-resolution images and at small sizes. Each marker features a unique ID number hence the marker IDs can be used to separate particular objects from others. In practice, these markers are placed in an environment to provide easily detectable visual cues and are used in applications like indoor tracking, robot navigation, augmented reality. These properties of fiducial markers make it a very suitable technology to use for block detection. 
Therefore, in this project, I am using fiducial markers to enable my system to detect the different programming blocks and create a flashing light sequence, which translates to a set of actions completed by the Ozobot. The markers also allow my system to execute the block recognition feature, where the system plays an audio file describing the action associated with the detected fiducial marker.  Before adopting fiducial markers in my system, I have considered different marker systems and eventually decided to employ arUco markers. In the following section, I will describe why the Aruco library is the most suitable for my system instead of other fidual marker systems.

\subsection{Types of Fiducial Marker Systems}
\subsubsection{ARToolkit}
ARToolkit markers consist of a square black border with a variety of different patterns in the interior. The quadrilateral black outline and the pattern is sampled to an element feature vector which is compared by correlation to a library of known markers. ARToolkit outputs a so called confidence factor. The presence of a marker is simply determined by a threshold value on this confidence value.
ARToolkit is useful for many applications, but has a few drawbacks. The use of correlation to identify markers causes high false positive and inter-marker confusion rates. The user typically has to capture each marker as seen with the camera and lighting of their application, plus adjust the greyscale threshold in a trade off between these two rates and the false negative detection rate. This makes this marker system unsuitable for my application. This method is rather disadvantageous as it requires markers to be captured at a high enough resolution so that no detail of the characters is lost. 

\subsubsection{ARTag}
ARTag makers feature a black and white block shape pattern inside a dark black border, as seen in figure 3.4. ARTag can  effectively detect a tag if part of the border is occluded, making it more accurate than ARToolkit, however the ARTag system constantly has a lower detection rate compared to AprilTag, ArUco and Stag markers [ref]. A study comparing the detection accuracy of these markers notes that ARTag consistently has a much lower detection rate hovering around \texttt{45\%}, while the AprilTag, ArUco, and STag markers have detection rates greater than \texttt{90\%} at different angles and resolutions of the camera. Since my system makes use of webcams that may have low resolutions, the ARTag is unsuitable for my system.

\subsubsection{AprilTag}
The AprilTag system was developed as an improved ARTag system. It was designed to[1]  Minimize false positive confusion rate and to minimize the number of bits per tag.
Based off experimental data, AprilTags was a significant improvement over prior methods such as QR codes or the ARToolKit[4]. Notably, AprilTags are significantly better at re- ducing false positives, specifically those which occur due to either part of the tag being covered, lighting issues, or errors due to color. Essentially, the algorithm will identify a tag incorrectly or may not identify it at all, which can be catastrophic in instances using robots, or break an augmented reality experience. However, the AprilTag is a computationally expensive package, and AprilTag markers are not built into the OpenCV library and require implementing their personal detection algorithm. Additionally, from an implementation perspective, ArUco marker detections tend to be more accurate, even when using the default parameters. On the other hand, ArUco markers are have online generators, that generate markers of different dictionaries and IDs. Hence, it is relatively straightforward to replace markers if they get damaged by generating them online and printing them. This cannot be done with AprilTags.

\subsubsection{ChromaTag}
ChromaTag is a fiducial marker system and detection algorithm designed to use opponent colors. Its design enables it to limit and quickly reject initial false detections and grayscale for precise localization. ChromaTag is proven to be significantly faster than current fiducial markers while achieving similar or better detection accuracy. However, it requires printing colored markers. However, using low resolution grey-scale webcams, or printing using uncalibrated colors, could lead to misdetection of markers. Since my system is utilized by children and teachers who may not have the right material for optimal detection, this marker is unsuitable for this project. 

\subsubsection{ArUco}
ArUco [17], is another package based on ARTag and ARToolkit. Unlike other fiducial marker systems, ArUco is that it allows the user to create configurable libraries, based on their specific needs. The library will only contain the specified number of markers with the greatest Hamming distance. Another notable contribution in ArUco is the reduced computing time due the smaller size of the custom libraries. It also provides a high detection rate with low rate of false positives, regardless of camera resolution or camera position. 
Other primary benefits of using ArUco markers over other markers include:

\begin{itemize}
  
\item ArUco markers are built into the OpenCV library via the cv2.aruco  submodule, described in section 
\item	The OpenCV library can generate ArUco markers with the cv2.aruco.drawMarker
 function.

\item	There are online ArUco generators.
For these reasons, I have chosen the ArUco marker system. It would also make my system more durable, since markers can be generated online without any code, and therefore easily replaced in case of damage.
\end{itemize}

\section{Image Processing} 
\subsection{imutils}

\noindent
Imutils are a series of functions to make image processing functions such as translation, rotation, resizing, skeletonization, and displaying images easier with OpenCV and both Python 2.7 and Python 3. Therefore, I am using imutils to manipulate live streamed images before detecting the markers using OpenCV. The following imutils functions are used:

\subsubsection{VideoStream}
With VideoStream, I have defined that source camera will be “0” which here corresponds to the webcam plugged to the computer port. After that, I have started the camera and defined that after start is initialized, the camera will turn on after 3 seconds so that camera sensor can warm up and image resolution is improved. From there, the .read() function is used to obtain the current frame captured by the webcam. The system continues looping over the frames of the live stream and computes live marker detection with 
OpenCV.

\subsubsection{Resize}
The resize function grabs the frame from the threaded video stream and resizes it to have a maximum width of 1000 pixels as the aruco detection algorithm requires.

\subsection{OpenCV}

To analyse the frames rendered by the imutils library, I have used the OpenCV library. OpenCV is a programming function library primarily used for Computer Vision applications. The three functions I have used to generate and detect markers obtained from my webcam are described here. 

\subsubsection{Drawmarker()}

Before their detection, ArUco markers need to be generated then printed in order to be placed in the environment. The OpenCV library has a built-in ArUco marker generator through its 
cv2.aruco.drawMarker function.The main parameters of this function is the marker dictionary  and the ID of the marker which has to be a valid ID In the ArUco dictionary. By specifying the marker dictionary and the marker ID, the drawMarker function then returns the output image with the ArUco marker drawn on it. 

There are 21 different ArUco dictionaries built into OpenCV. The majority of these dictionaries follow a specific naming convention, \texttt{cv2.aruco.DICT\_NxN\_M}, with an NxN size followed by an integer value, M. For example \texttt{DICT\_6X6\_250} means this dictionary is composed of 250 markers and a marker size of 6x6 bits. There are some settings to consider in order to choose the ideal dictionary, and these differ from system to system. It is generally more ideal to pick a dictionary with a larger NxN grid size, balanced with a low number of unique ArUco IDs such that the inter-marker distance can be used to correct misread markers. 
Therefore, for my system, I have chosen the \texttt{DICT\_7X7\_50}, this is because my system requires 15 unique IDs for 15 different actions, and this dictionary has the smallest number of unique IDs (50 IDs). The marker size of 7x7 bits also provides more accurate detection results than other marker sizes due to the larger grid size and inter-marker distance. 
I have generated a total of 15 markers for my system with ID’s in the range of [1,15], with one increment. 

\subsubsection{Detectmarker()}

The OpenCV library is used to detect the markers in the frames rendered by the imutils video library. I have made use of OpenCV’s cv2.aruco module specifically made to detect aruco markers. The marker detection process is comprised of two main steps:
\begin{enumerate}
    \item Detection of marker candidates. In this step, the frame read by the VideoStream.read() function described in 4.3.1. is analyzed in order to find square shapes that are candidates to be markers. It starts with an adaptive thresholding to segment the markers, then outlines are extracted from the thresholded image and those that do not approximate to a square shape are discarded. Some extra filtering is also applied to removing contours that are too small, too big, or too close to each other.
    \item After the candidate detection, it is necessary to determine if they are actually markers by analyzing their inner codification. This step starts by extracting the marker bits of each marker. The image is divided into different cells according to the marker size and the border size. Then the number of black or white pixels in each cell is counted to determine if it is a white or a black bit. Finally, the bits are analyzed to determine if the marker belongs to the specific dictionary. Error correction techniques are employed when necessary.
\end{enumerate}
I have designed my system to specifically detect markers from the dictionary \texttt{DICT\_7X7\_50} as specified in the previous paragraph. Since my system makes use of a live stream video from the webcam, I have included code to draw the outlines, the center and the ID of the detected tag on the screen to show real time detection. 
Finally, the IDs of the detected markers are stored in a list and used to translate to a flashing light sequence of instructions or parsed to the block recognition code, that plays an audio file depending on the block recognized. The working of this will be explained in more detail in the technical design chapter.

\section{playsound}

To implement the block recognition feature, I have used the playsound Python module. This enables my system to play mp3 audio files describing the action on the block detected by the OpenCV detection algorithm. My code performs this by detecting the marker ID, each marker ID has been assigned to an action. 
\begin{table}[t]
\centering
\begin{tabular}{|cc|c|}
\hline
Marker ID      & Action Associated      & Audio Output      \\
\hline
$1     $ & $\texttt{Start program} $ & $\texttt{'Start Block'}    $ \\
$3     $ & $\texttt{Turn Left     }$ & $\texttt{'Make a Left Turn'}   $ \\
$5     $ & $\texttt{U-turn Left}   $ & $ \texttt{'Make a U-turn to the left'} $\\
$7     $ & $\texttt{Move Forward}  $ & $\texttt{'Move Forward Block'}   $ \\
$9     $ & $\texttt{Variable Block: 5 steps}  $ & $\texttt{'Move for 5 steps}'  $ \\
\hline
\end{tabular}
\caption{Examples of markers and their corresponding actions.}
\label{tab}
\end{table}

\section{Tkinter}

To build my GUI, I have utilised the GUI Python Framework Tkinter and its functions to allow the user to interact with my system.

\subsection{Frame}
My GUI design involves a frame widget, it works on grouping and organizing other widgets by arranging their position. This widget is essential in my system since it allows me to choose the background color and the size of the displayed windows. For example, the parent window, which is the window displayed when the system starts, includes an organised layout of a label widget and different button widgets. The system displays a new frame depending on the buttons clicked. Hence, I have employed the frame widget as a foundation class to implement widgets and different windows.

\subsection{Button}
I have implemented the tkinter button widget in my system. There are two buttons that appear on the main window when the application starts. The buttons are linked to different functions in my code and perform the required actions depending on the button clicked.

\subsection{Label}
The label widget was used to display information such as descriptions or instructions at the top of the screen once the GUI has been loaded. I have also employed this widget to display any error messages that the system detects.

\section{Ozopy}
To interpret the detected markers and their associated instructions and convert them into flashing light sequence for the Ozobot, I have implemented an open-source Python-like language compiler for Ozobot [ref] called Ozopy, invented by Kaarl Ma..  This Python-like language is a subset of Python with some added builtin methods to control the Ozobot bit.

\subsection{Context}

The Ozobot can be programmed in two ways, either by using color codes and colored markers to indicate actions, or by using the visual programming interface OzoBlockly. With OzoBlockly, the user can create programs for the Ozobot by dragging puzzle shaped blocks onto the programming platform. The on-screen blocks clip together to form a program. The OzoBlockly editor has five skills levels from pre-reader to master.
For my system’s purposes, I will be focusing on level 1 and level 2, which are pre-reader and beginner levels. This is due to there being very few accessible programming interfaces for younger demographics. The level 1 blocks feature simple shapes depicting movements such as arrows, while level 2 blocks feature the same blocks but with written statements instead of icons with the addition of loops. 

To convert the created program into instructions understood by the Ozobot, OzoBlocky displays the encoded values as a light sequence composed of 8 color constants: black, white, red, green, yellow, blue, magenta and cyan. This is scanned at the base of the Ozobot where the RGB colour sensors reside. 

The engineering of the colour codes for OzoBlockly is not publicly available. However, Kaarel Maidre and Ashley Feniello have successfully reverse engineered the colour codes and created Ozopy and FlashForth, programming IDE for the Ozobot. The following section explains the working of Ozopy.  

\subsection{Encoding Values}
Ozopy functions similarly to OzoBlockly. The language compiler interprets a set of instructions written on an .ozopy file and outputs a window with flashing lights  of colour codes. These colour codes translate to bytecode instructions that the Ozobot executes. Ashley Fenielo, who made FlashForth, has successfully decoded the colour sequences and established their corresponding bytecodes. Her work represents the groundwork behind Ozopy.

The color codes are encoded in a base-7 encoding and line up on byte-sized boundaries endoded as sets of three colours. Each of the eight colors has a 3-bit value. The following table shows the denary value of the colors and their 3-bit encoding.



A set of three consecutives colors encode a bytecode used to program the Ozobot, except for the color White which is not being used as a value but rather signifies repeating the last color. For example, KWK is simply 000.

Each of the three color codes is translated to a decimal number. The resulting number is then converted to hexadecimal to produce a bytecode that can be used to lookup instructions that are featured on the Ozobot. 

\subsection{Instructions}
After the color codes are translated to bytecodes, the resulting bytecode value is used to lookup instructions. As the Ozobot functions as a stack machine, operands are sent before operations.
Values of less than 128 are considered literals pushed to the stack, while values of 128 or higher are instructions. The table created by Feniello shows the bytecodes and their matched instructions. 


\section{Summary}
% -----------------------------------------------------------------------------
\chapter{Exploring Accessible Programming}
\section{Study}

% -----------------------------------------------------------------------------
\chapter{System Design}
Considering the advantages of the existing approaches to teach computational concepts discussed in previous chapters, my research with SNEs and QTVIs, I designed BOP - a Block-based Ozobot Programming system. In this chapter, I present the design and implementation of the different components of BOP.

\section{Architecture}
My system is composed of a set of tangible progamming blocks, an Ozobot robot, a computer application, a tripod, and a workspace. Students can manipulate the tangible blocks to construct a program with the desired actions for the robot to execute. The blocks are placed in the workspace underneath the webcam device held by the tripod. The webcam is connected to a computer, where the designed application can recognize the ArUco marker in the blocks and translate them to a flashing light sequence. The Ozobot is held up to the screen to scan the flashing light and then execute the dictated actions.

In the following section, I analyze the architecture in further detail, from a physical design and a technical design point of view.

\section{Physical Design}

\section{Technical Design}
The technical design of my system converts the programming blocks scanned by the webcam to a flashing light sequence, used to transmit instructions to the Ozobot or alternatively performs block recognition by reading out the actions displayed on the blocks. The different components interact with each other to program the Ozobot through five main steps: live stream video of blocks, detection of markers, ordering of marker ID, translation to Ozopy, and output light sequence. On the other hand, for block recognition, the components interact differently and fewer steps are required. In this section, I will discuss in more detail each of these steps and their technical aspects. 
 
\subsection{GUI design} 
 
The GUI is designed in a way that when the system starts, the user is presented with two buttons, Translate to Ozopy and Recognise Block. The translate button captures the blocks on the workspace using the webcam attached to the tripod and translates it to the Ozopy language, and the ‘recognise block’ button which plays an audio file. When clicked on, both buttons display a window which showcases the current input from the webcam.  
I have chosen to implement a GUI framework because it is a very intuitive platform and does not require the use of the terminal or command line. Since this system is primarly designed for teachers of the students, it will be very straightforward to use. It also outputs separate windows depending on the command chosen, this feature avoids any confusion due to clustering of windows when using the system.  
GUIs can also be made accessible depending on the type of computer. For example, with Apple computers, the user can make use of the voiceover application to read the text displayed on the tkinter window. (Maybe put in technical background) 
  
\subsection{Detection} 
 
Right before the detection stage, the program loops over the frames of the live streamed video and reads in the images using the imutils library previously mentionned. The detection stage is where the rendered frames are analysed using the OpenCV aruco module and the detectmarker() function described in chapter . 
The detection process outputs a list of markers and their corresponding IDs. 
After the detection process, the IDs are utilised to either identify a block or translate instructions for the Ozobot. 
 
\subsection{Block Recognition}
 
This process simply involves using the detected marker from the live streamed video to look up prewritten lines of code matching the marker ID to an audio file and finally playing that audio. 
 
\subsection{Ordering} 
 
After the blocks are detected and their marker IDs stored in a list, the system orders the markers IDs based on their distance from the start block. This is to ensure the output program is formed in the right order. The start block is fixed at the base of the tripod and is placed at the start of the workspace. Therefore, using the four detected corners from the detection algorithm, the system calculates the (x,y) coordinates of the center of the start block and repeats that calculation to every marker. After obtainging the coordinates of all the centers, the coordinates are stored in a list and sorted incrementally compared to the center of the start block. 
The system checks that the first and last marker IDs correspond to the end and start blocks and notifies the user of the issue if not. This alert is outputted as an audio file, to make the debugging of the program accessible to all visual abilities.  
//ordering of variables// 
 
\subsection{Translation to Ozopy}
 
In this stage, the list of marker IDs is translated to code used by the Ozopy language compiler. My system iterates through the ordered marker IDs and uses the ID list to look up pre-written lines of code. Each ID has been matched to describe a command. When this ID is identified in the list, its corresponding command is written to a text file called code.ozopy.  
When the system detects a start loop block marker ID, the commands coming after the loop are indented inside the text file, and then when the end loop block is detected, the indentation is decreased. The loop is created using the while command and a variable is initialized and incremented to track the number of iterations. The number of iterations is indicated by the variable block slotted in the loop block. The program is terminated when the end marker ID is translated to the ‘terminate(OFF)’ function. This command switches the Ozobot off. 
The lines of commands written to code.ozopy are then ready to be turned to a flashing light sequence. 
 
 
\subsection{Output}  
This is the last step in the ‘Translate to Ozopy’ workflow. This stage uses the Ozopy language library to turn the list of commands to a light sequence. The library is run using the command ozopython.run and the name of the file containing the commands code.ozopy. The ozopython library reads in the lines in the file and converts them to bytecodes. The bytecodes are then converted to color codes and a small window appears on the screen. The window features a white background that allows calibration of the Ozobot, and a load button which plays the sequence of flashing lights. The Ozobot is placed on the screen, with the base of the robot where the sensors reside, facing the load window. It takes in the sequence of colors and then performs a set of actions. 

\section{Summary}









% -----------------------------------------------------------------------------
\chapter{Post Prototype User Study}

% -----------------------------------------------------------------------------

\chapter{Conclusion and Further Work}
\label{chap:conclusion}

\noindent
The concluding chapter of a dissertation is often underutilised because it 
is too often left too close to the deadline: it is important to allocate
enough attention to it.  Ideally, the chapter will consist of three parts:

\begin{enumerate}
\item (Re)summarise the main contributions and achievements, in essence
      summing up the content.
\item Clearly state the current project status (e.g., ``X is working, Y 
      is not'') and evaluate what has been achieved with respect to the 
      initial aims and objectives (e.g., ``I completed aim X outlined 
      previously, the evidence for this is within Chapter Y'').  There 
      is no problem including aims which were not completed, but it is 
      important to evaluate and/or justify why this is the case.
\item Outline any open problems or future plans.  Rather than treat this
      only as an exercise in what you {\em could} have done given more 
      time, try to focus on any unexplored options or interesting outcomes
      (e.g., ``my experiment for X gave counter-intuitive results, this 
      could be because Y and would form an interesting area for further 
      study'' or ``users found feature Z of my software difficult to use,
      which is obvious in hindsight but not during at design stage; to 
      resolve this, I could clearly apply the technique of Smith [7]'').
\end{enumerate}

% =============================================================================

% Finally, after the main matter, the back matter is specified.  This is
% typically populated with just the bibliography.  LaTeX deals with these
% in one of two ways, namely
%
% - inline, which roughly means the author specifies entries using the 
%   \bibitem macro and typesets them manually, or
% - using BiBTeX, which means entries are contained in a separate file
%   (which is essentially a databased) then inported; this is the 
%   approach used below, with the databased being dissertation.bib.
%
% Either way, the each entry has a key (or identifier) which can be used
% in the main matter to cite it, e.g., \cite{X}, \cite[Chapter 2}{Y}.
%
% We would recommend using BiBTeX, since it guarantees a consistent referencing style 
% and since many sites (such as dblp) provide references in BiBTeX format. 
% However, note that by default, BiBTeX will ignore capital letters in article titles 
% to ensure consistency of style. This can lead to e.g. "NP-completeness" becoming
% "np-completeness". To avoid this, make sure any capital letters you want to preserve
% are enclosed in braces in the .bib, e.g. "{NP}-completeness".

\backmatter

\bibliography{dissertation}

% -----------------------------------------------------------------------------

% The dissertation concludes with a set of (optional) appendicies; these are 
% the same as chapters in a sense, but once signaled as being appendicies via
% the associated macro, LaTeX manages them appropriatly.

\appendix

\chapter{An Example Appendix}
\label{appx:example}

Content which is not central to, but may enhance the dissertation can be 
included in one or more appendices; examples include, but are not limited
to

\begin{itemize}
\item lengthy mathematical proofs, numerical or graphical results which 
      are summarised in the main body,
\item sample or example calculations, 
      and
\item results of user studies or questionnaires.
\end{itemize}

\noindent
Note that in line with most research conferences, the marking panel is not
obliged to read such appendices. The point of including them is to serve as
an additional reference if and only if the marker needs it in order to check
something in the main text. For example, the marker might check a program listing 
in an appendix if they think the description in the main dissertation is ambiguous.

% =============================================================================

\end{document}
